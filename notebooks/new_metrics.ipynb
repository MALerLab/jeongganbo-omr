{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dongmin/userdata/jeongganbo-omr\n"
     ]
    }
   ],
   "source": [
    "%cd /home/dongmin/23FW-NCG/jeongganbo-omr\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongmin/.local/share/virtualenvs/jeongganbo-omr-QQMBMPoZ/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import csv\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "from random import randint, choice, uniform, seed\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from exp_utils import JeongganSynthesizer, get_img_paths\n",
    "from exp_utils.model_zoo import OMRModel, TransformerOMR\n",
    "from exp_utils.train_utils import Trainer, Dataset, Tokenizer, pad_collate, LabelStudioDataset, draw_low_confidence_plot\n",
    "\n",
    "dprint = lambda d: print(json.dumps(d, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,\n",
       "       0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,\n",
       "       0.03571429, 0.03571429, 0.17857143, 0.03571429, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
       "       0.01785714, 0.01785714, 0.01785714, 0.01785714])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from exp_utils import SYMBOL_W_DUR_EN_LIST, JeongganSynthesizer\n",
    "\n",
    "len(SYMBOL_W_DUR_EN_LIST), len(JeongganSynthesizer.get_pitch_range())\n",
    "\n",
    "p_prob = [1] * 12 + [5, 1] + [0.5] * len(SYMBOL_W_DUR_EN_LIST)\n",
    "np.asarray(p_prob) / sum(p_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.42857147999999995, 0.3571427999999999)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,0.03571429, 0.03571429, 0.03571429, 0.03571429, 0.03571429,0.03571429, 0.03571429]), \\\n",
    "sum([0.01785714,\n",
    "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
    "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
    "       0.01785714, 0.01785714, 0.01785714, 0.01785714, 0.01785714,\n",
    "       0.01785714, 0.01785714, 0.01785714, 0.01785714])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipline mock-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "load tokenizer\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPELETE: Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# random seed setting\n",
    "project_root_dir = Path('.')\n",
    "output_dir = project_root_dir / 'outputs'\n",
    "\n",
    "dir_date = '2024-05-04'\n",
    "dir_time = '20-31-38'\n",
    "\n",
    "exp_dir = output_dir / dir_date / dir_time\n",
    "\n",
    "conf = OmegaConf.load(exp_dir / '.hydra' / 'config.yaml')\n",
    "device = torch.device(conf.general.device)\n",
    "\n",
    "model_dir = exp_dir / 'model'\n",
    "\n",
    "print('\\nload tokenizer')\n",
    "\n",
    "tokenizer_vocab_fn = model_dir / f'{conf.general.model_name}_tokenizer.txt'\n",
    "tokenizer = Tokenizer(vocab_txt_fn=tokenizer_vocab_fn)\n",
    "\n",
    "print('COMPLETE: load tokenizer')\n",
    "\n",
    "\n",
    "print('\\ndata_set loading...')\n",
    "\n",
    "test_set = LabelStudioDataset(project_root_dir / conf.data_path.test, project_root_dir / 'jeongganbo-png/splited-pngs', remove_borders=conf.test_setting.remove_borders, is_valid=True)\n",
    "\n",
    "test_set.tokenizer = tokenizer\n",
    "\n",
    "test_loader = DataLoader(test_set, batch_size=1000, shuffle=False, collate_fn=pad_collate, num_workers=conf.dataloader.num_workers_load)\n",
    "\n",
    "print('COMPLETE: data_set loading')\n",
    "\n",
    "\n",
    "print('\\nmodel initializing...')\n",
    "model = TransformerOMR(conf.model.dim, len(tokenizer.vocab), enc_depth=conf.model.enc_depth, dec_depth=conf.model.dec_depth, num_heads=conf.model.num_heads, dropout=conf.model.dropout)\n",
    "model.load_state_dict(torch.load(model_dir / f'{conf.general.model_name}_HL_{conf.test_setting.target_metric}_best.pt', map_location='cpu')['model'])\n",
    "\n",
    "tester = Trainer(model, \n",
    "                 None, #optimizer\n",
    "                 None, #loss_fn\n",
    "                 None, #train_loader\n",
    "                 test_loader, \n",
    "                 tokenizer,\n",
    "                 device=device, \n",
    "                 scheduler=None,\n",
    "                 aux_loader=None,\n",
    "                 aux_freq=None,\n",
    "                 mix_aux=None,\n",
    "                 aux_valid_loader=None,\n",
    "                 wandb=None, \n",
    "                 model_name=conf.general.model_name,\n",
    "                 model_save_path=model_dir,\n",
    "                 checkpoint_logger=None)\n",
    "\n",
    "print('COMPLETE: model initializing')\n",
    "\n",
    "\n",
    "print('\\nTesting...')\n",
    "\n",
    "_, _, pred_tensor_list, confidence_tensor_list = tester.validate(with_confidence=True)\n",
    "\n",
    "print('COMPELETE: Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "\n",
    "for b_idx in range(len(pred_tensor_list)):\n",
    "  pred_list += pred_tensor_list[b_idx].tolist()\n",
    "\n",
    "pred_list = list( enumerate(pred_list) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_specials = lambda seq: [ x for x in seq if x not in (0, 1, 2) ]\n",
    "\n",
    "def calc_acc(gt, prd):\n",
    "  gt, prd = torch.tensor(gt, dtype=torch.long), torch.tensor(prd, dtype=torch.long)\n",
    "  \n",
    "  if prd.shape[0] > gt.shape[0]:\n",
    "    prd = prd[:gt.shape[0]]\n",
    "  elif prd.shape[0] < gt.shape[0]:\n",
    "    prd = torch.cat([prd, torch.zeros(gt.shape[0] - prd.shape[0])], dim=-1)\n",
    "  \n",
    "  num_tokens = gt.shape[0]\n",
    "  num_match_token = prd == gt\n",
    "  num_match_token = num_match_token[gt != 0].sum().item()\n",
    "  \n",
    "  return num_match_token / num_tokens\n",
    "\n",
    "def make_gt_pred_tuple_list(_tokenizer, ls, with_img=False):\n",
    "  tup_list = []\n",
    "  \n",
    "  for didx, pred in ls:\n",
    "    img, label = test_set.get_item_by_idx(didx)\n",
    "    \n",
    "    if not with_img:\n",
    "      img = None\n",
    "    \n",
    "    label_enc = _tokenizer(label)[1:-1]\n",
    "    pred = filter_specials(pred)\n",
    "    \n",
    "    acc = calc_acc(label_enc, pred)\n",
    "    \n",
    "    tup_list.append( (didx, img, label_enc, pred, acc) )\n",
    "  \n",
    "  return tup_list\n",
    "\n",
    "pred_list = make_gt_pred_tuple_list(tokenizer, pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1281"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_list_long = list( filter( lambda x: len(x[2]) > 2, pred_list ) )\n",
    "\n",
    "len(pred_list_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list_long = sorted(pred_list_long, key=lambda x: x[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed(1)\n",
    "\n",
    "test_ls = []\n",
    "\n",
    "for i in range(11):\n",
    "  f = [t for t in pred_list_long if t[4] < (i + 1)*0.1 and t[4] > i*0.1]\n",
    "  test_ls += [choice(f)] if len(f) > 0 else []\n",
    "\n",
    "len(test_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(402,\n",
       "  None,\n",
       "  [119, 34, 12, 3, 4, 14, 3, 89, 16, 3, 121, 18],\n",
       "  [119, 12, 3, 4, 14, 3, 89, 16, 3, 121, 18],\n",
       "  0.08333333333333333),\n",
       " (1218,\n",
       "  None,\n",
       "  [76, 52, 6, 3, 127, 58, 7],\n",
       "  [76, 6, 3, 127, 58, 7],\n",
       "  0.14285714285714285),\n",
       " (1333,\n",
       "  None,\n",
       "  [140, 12, 3, 4, 15, 3, 127, 49, 18],\n",
       "  [140, 6, 3, 127, 49, 7],\n",
       "  0.2222222222222222),\n",
       " (1356, None, [140, 55, 15], [140, 15], 0.3333333333333333),\n",
       " (370,\n",
       "  None,\n",
       "  [80, 12, 3, 4, 14, 3, 87, 57, 16, 3, 83, 18],\n",
       "  [80, 57, 12, 3, 4, 14, 3, 87, 16, 3, 83, 18],\n",
       "  0.4166666666666667),\n",
       " (1494,\n",
       "  None,\n",
       "  [140, 12, 3, 4, 57, 15, 3, 106, 18],\n",
       "  [140, 57, 12, 3, 4, 15, 3, 106, 18],\n",
       "  0.5555555555555556),\n",
       " (951, None, [140, 67, 15], [140, 68, 15], 0.6666666666666666),\n",
       " (1247,\n",
       "  None,\n",
       "  [127, 12, 3, 4, 15, 3, 4, 17, 3, 141, 47, 19],\n",
       "  [127, 12, 3, 4, 15, 3, 4, 17, 3, 87, 19],\n",
       "  0.75),\n",
       " (1529,\n",
       "  None,\n",
       "  [4, 12, 3, 4, 57, 15, 3, 127, 18],\n",
       "  [4, 12, 3, 4, 57, 15, 3, 127, 64, 18],\n",
       "  0.8888888888888888),\n",
       " (1347,\n",
       "  None,\n",
       "  [105, 12, 3, 4, 14, 3, 127, 16, 3, 105, 17, 3, 140, 55, 19],\n",
       "  [105, 12, 3, 4, 14, 3, 127, 16, 3, 4, 17, 3, 140, 55, 19, 3, 83],\n",
       "  0.9333333333333333)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New metrics\n",
    "* NER: note error rate\n",
    "* PER: position error rate\n",
    "* NPER: note-position pair error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청남_노네:2 -:4 니나*:6 청임:8\n",
      "[('_노네', '2')]\n",
      "[('_노네', '청남', '2')]\n",
      "\n",
      "고_덧길이표:10 태_반길이표:11\n",
      "[('_덧길이표', '10'), ('_반길이표', '11')]\n",
      "[('_덧길이표', '고', '10'), ('_반길이표', '태', '11')]\n",
      "\n",
      "황:2 -:5 태_니레:8\n",
      "[('_니레', '8')]\n",
      "[('_니레', '태', '8')]\n",
      "\n",
      "황_루러표:5\n",
      "[('_루러표', '5')]\n",
      "[('_루러표', '황', '5')]\n",
      "\n",
      "노:2 -:4 니_미는표:6 느나:8\n",
      "[('_미는표', '6')]\n",
      "[('_미는표', '니', '6')]\n",
      "\n",
      "황:2 -_미는표:5 배중:8\n",
      "[('_미는표', '5')]\n",
      "[('_미는표', '-', '5')]\n",
      "\n",
      "황_자출:5\n",
      "[('_자출', '5')]\n",
      "[('_자출', '황', '5')]\n",
      "\n",
      "태:2 -:5 -:7 흘림표_니:9\n",
      "[('_니', '9')]\n",
      "[('_니', '흘림표', '9')]\n",
      "\n",
      "-:2 -_미는표:5 태:8\n",
      "[('_미는표', '5')]\n",
      "[('_미는표', '-', '5')]\n",
      "\n",
      "배임:2 -:4 태:6 배임:7 황_루러표:9\n",
      "[('_루러표', '9')]\n",
      "[('_루러표', '황', '9')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_notes_and_positions(label):\n",
    "  pattern = r'([^_\\s:]+|_+[^_\\s:]+|:\\d+|[-])'\n",
    "  \n",
    "  def clean_findings(fds):\n",
    "    int_idx = []\n",
    "    \n",
    "    for i, t in enumerate(fds):\n",
    "      try:\n",
    "        int(t[1:]) # if str p cannot be cast into integer goto except block\n",
    "        int_idx.append(i)\n",
    "      except:\n",
    "        pass\n",
    "    \n",
    "    if len(int_idx) < 1:\n",
    "      return [fds]\n",
    "    \n",
    "    split_fds = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    for i in int_idx:\n",
    "      end_idx = i + 1\n",
    "      split_fds.append(fds[start_idx:end_idx])\n",
    "      start_idx = end_idx\n",
    "    \n",
    "    return split_fds\n",
    "\n",
    "  token_groups = label.split()\n",
    "\n",
    "  notes = []\n",
    "  positions = []\n",
    "  ornaments = []\n",
    "\n",
    "  for group in token_groups:\n",
    "    findings = re.findall(pattern, group)\n",
    "    findings = clean_findings(findings)\n",
    "    \n",
    "    for fd in findings:\n",
    "      note = fd[0]\n",
    "      position = fd[-1]\n",
    "      ornament = fd[1:-1]\n",
    "      \n",
    "      if note[0] == '_':\n",
    "        ornament = [note] + ornament\n",
    "        note = '<pad>'\n",
    "        \n",
    "      notes.append(note)\n",
    "      positions.append(position[1:])\n",
    "      ornaments.append(ornament)\n",
    "\n",
    "  return notes, positions, ornaments\n",
    "\n",
    "for test_case in test_ls:\n",
    "  label, pred = test_case[2:4]\n",
    "  \n",
    "  label_dec = tokenizer.decode( label )\n",
    "  pred_dec = tokenizer.decode( pred )\n",
    "  \n",
    "  label_note, label_pos, label_ornament = get_notes_and_positions(label_dec)\n",
    "  pred_note, pred_pos, pred_ornament = get_notes_and_positions(pred_dec)\n",
    "  \n",
    "  label_note_pos = list( zip(label_note, label_pos) )\n",
    "  pred_note_pos = list( zip(pred_note, pred_pos) )\n",
    "  \n",
    "  map_ornamnet_pos = lambda ol, pl: [(o, p) for i, p in enumerate(pl) for o in ol[i]]\n",
    "  map_ornament_note_pos = lambda ol, tl: [ (o, *t) for i, t in enumerate(tl) for o in ol[i]]\n",
    "  \n",
    "  print(label_dec)\n",
    "  print(map_ornamnet_pos(label_ornament, label_pos))\n",
    "  print(map_ornament_note_pos(label_ornament, label_note_pos))\n",
    "  \n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임_노니로:1 중:3 -:5 무_느니-르_노니로:8_느니-르_노니로:8\n",
      "(['임', '중', '-', '무', '<pad>'], ['1', '3', '5', '8', '8'], [['_노니로'], [], [], ['_느니-르', '_노니로'], ['_느니-르', '_노니로']])\n",
      "황:1 태:3 -:5 -:7 배임_노니로:9_노니로:9\n",
      "(['황', '태', '-', '-', '배임', '<pad>'], ['1', '3', '5', '7', '9', '9'], [[], [], [], [], ['_노니로'], ['_노니로']])\n",
      "황:1 태:3 -_흘림표:5 -:8 니:9니:9\n",
      "(['황', '태', '-', '-', '니', '니'], ['1', '3', '5', '8', '9', '9'], [[], [], ['_흘림표'], [], [], []])\n",
      "태:1 황:3 -:4 니:6 배남_흘림표:8_흘림표:8\n",
      "(['태', '황', '-', '니', '배남', '<pad>'], ['1', '3', '4', '6', '8', '8'], [[], [], [], [], ['_흘림표'], ['_흘림표']])\n"
     ]
    }
   ],
   "source": [
    "test_lbs = [\n",
    "  '임_노니로:1 중:3 -:5 무_느니-르_노니로:8_느니-르_노니로:8',\n",
    "  '황:1 태:3 -:5 -:7 배임_노니로:9_노니로:9',\n",
    "  '황:1 태:3 -_흘림표:5 -:8 니:9니:9',\n",
    "  '태:1 황:3 -:4 니:6 배남_흘림표:8_흘림표:8',\n",
    "]\n",
    "\n",
    "for lb in test_lbs:\n",
    "  print(lb)\n",
    "  print(get_notes_and_positions(lb))\n",
    "\n",
    "# print(test_lbs[2])\n",
    "# get_notes_and_positions(test_lbs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청남_노네:2 -:4 니나*:6 청임:8\n",
      "['청남', '-', '니나*', '청임']\n",
      "[':2', ':4', ':6', ':8']\n",
      "['_노네']\n",
      "청남:2 -:4 니나*:6 청임:8\n",
      "['청남', '-', '니나*', '청임']\n",
      "[':2', ':4', ':6', ':8']\n",
      "[]\n",
      "\n",
      "고_덧길이표:10 태_반길이표:11\n",
      "['고', '태']\n",
      "[':10', ':11']\n",
      "['_덧길이표', '_반길이표']\n",
      "고:10 태_반길이표:11\n",
      "['고', '태']\n",
      "[':10', ':11']\n",
      "['_반길이표']\n",
      "\n",
      "황:2 -:5 태_니레:8\n",
      "['황', '-', '태']\n",
      "[':2', ':5', ':8']\n",
      "['_니레']\n",
      "황:10 태_니레:11\n",
      "['황', '태']\n",
      "[':10', ':11']\n",
      "['_니레']\n",
      "\n",
      "황_루러표:5\n",
      "['황']\n",
      "[':5']\n",
      "['_루러표']\n",
      "황:5\n",
      "['황']\n",
      "[':5']\n",
      "[]\n",
      "\n",
      "노:2 -:4 니_미는표:6 느나:8\n",
      "['노', '-', '니', '느나']\n",
      "[':2', ':4', ':6', ':8']\n",
      "['_미는표']\n",
      "노_미는표:2 -:4 니:6 느나:8\n",
      "['노', '-', '니', '느나']\n",
      "[':2', ':4', ':6', ':8']\n",
      "['_미는표']\n",
      "\n",
      "황:2 -_미는표:5 배중:8\n",
      "['황', '-', '배중']\n",
      "[':2', ':5', ':8']\n",
      "['_미는표']\n",
      "황_미는표:2 -:5 배중:8\n",
      "['황', '-', '배중']\n",
      "[':2', ':5', ':8']\n",
      "['_미는표']\n",
      "\n",
      "황_자출:5\n",
      "['황']\n",
      "[':5']\n",
      "['_자출']\n",
      "황_전성:5\n",
      "['황']\n",
      "[':5']\n",
      "['_전성']\n",
      "\n",
      "태:2 -:5 -:7 흘림표_니:9\n",
      "['태', '-', '-', '흘림표']\n",
      "[':2', ':5', ':7', ':9']\n",
      "['_니']\n",
      "태:2 -:5 -:7 니:9\n",
      "['태', '-', '-', '니']\n",
      "[':2', ':5', ':7', ':9']\n",
      "[]\n",
      "\n",
      "-:2 -_미는표:5 태:8\n",
      "['-', '-', '태']\n",
      "[':2', ':5', ':8']\n",
      "['_미는표']\n",
      "-:2 -_미는표:5 태_시루표:8\n",
      "['-', '-', '태']\n",
      "[':2', ':5', ':8']\n",
      "['_미는표', '_시루표']\n",
      "\n",
      "배임:2 -:4 태:6 배임:7 황_루러표:9\n",
      "['배임', '-', '태', '배임', '황']\n",
      "[':2', ':4', ':6', ':7', ':9']\n",
      "['_루러표']\n",
      "배임:2 -:4 태:6 -:7 황_루러표:9 느나\n",
      "['배임', '-', '태', '-', '황', '느나']\n",
      "[':2', ':4', ':6', ':7', ':9', '<pad>']\n",
      "['_루러표']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_enc_seq(_tokenizer, enc_lb):\n",
    "  blank_tok_idx = _tokenizer.tok2idx[' ']\n",
    "  split_idx_list = [ i for i, t in enumerate(enc_lb) if t == blank_tok_idx ]\n",
    "  \n",
    "  is_ornament = lambda ti: '_' == _tokenizer.vocab[ti][0]\n",
    "  is_position = lambda ti: ':' == _tokenizer.vocab[ti][0]\n",
    "  \n",
    "  def clean_findings(gl):\n",
    "    il = [ i for i, t in enumerate(gl) if is_position(t) ]\n",
    "    \n",
    "    ls = []\n",
    "    st = 0\n",
    "    \n",
    "    for i in il + [None]:\n",
    "      ed = i + 1 if i else None\n",
    "      \n",
    "      sl = gl[st:ed]\n",
    "      \n",
    "      if len(sl) < 1:\n",
    "        continue\n",
    "      \n",
    "      if is_ornament(sl[0]):\n",
    "        sl = [0] + sl\n",
    "\n",
    "      if not is_position(sl[-1]):\n",
    "        sl = sl + [0]\n",
    "      \n",
    "      ls.append(sl)  \n",
    "      \n",
    "      st = ed\n",
    "    \n",
    "    return ls\n",
    "  \n",
    "  groups = []\n",
    "  split_st = 0\n",
    "  \n",
    "  for si in split_idx_list + [None]:\n",
    "    g = enc_lb[split_st:si]  \n",
    "    g = clean_findings(g)\n",
    "    \n",
    "    for subg in g:\n",
    "      note = subg[0]\n",
    "      position = subg[-1]\n",
    "      ornament = subg[1:-1]\n",
    "    \n",
    "      groups.append((note, position, ornament))\n",
    "    \n",
    "    if si:\n",
    "      split_st = si + 1\n",
    "  \n",
    "  return groups\n",
    "\n",
    "for test_case in test_ls:\n",
    "  label, pred = test_case[2:4]\n",
    "  \n",
    "  label_dec = tokenizer.decode( label )\n",
    "  pred_dec = tokenizer.decode( pred )\n",
    "  \n",
    "  label_split = split_enc_seq(tokenizer, label)\n",
    "  print(label_dec)\n",
    "  print([ tokenizer.vocab[g[0]] for g in label_split ])\n",
    "  print([ tokenizer.vocab[g[1]] for g in label_split ])\n",
    "  print([ tokenizer.vocab[orn] for g in label_split for orn in g[2] ])\n",
    "  \n",
    "  pred_split = split_enc_seq(tokenizer, pred)\n",
    "  print(pred_dec)\n",
    "  print([ tokenizer.vocab[g[0]] for g in pred_split ])\n",
    "  print([ tokenizer.vocab[g[1]] for g in pred_split ])\n",
    "  print([ tokenizer.vocab[orn] for g in pred_split for orn in g[2] ])\n",
    "  \n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임_노니로:1 중:3 -:5 무_느니-르_노니로:8_느니-르_노니로:8\n",
      "['임', '중', '-', '무', '<pad>']\n",
      "[':1', ':3', ':5', ':8', ':8']\n",
      "['_노니로', '_느니-르', '_노니로', '_느니-르', '_노니로']\n",
      "\n",
      "황:1 태:3 -:5 -:7 배임_노니로:9_노니로:9\n",
      "['황', '태', '-', '-', '배임', '<pad>']\n",
      "[':1', ':3', ':5', ':7', ':9', ':9']\n",
      "['_노니로', '_노니로']\n",
      "\n",
      "황:1 태:3 -_흘림표:5 -:8 니:9니:9\n",
      "['황', '태', '-', '-', '니', '니']\n",
      "[':1', ':3', ':5', ':8', ':9', ':9']\n",
      "['_흘림표']\n",
      "\n",
      "태:1 황:3 -:4 니:6 배남_흘림표:8_흘림표:8\n",
      "['태', '황', '-', '니', '배남', '<pad>']\n",
      "[':1', ':3', ':4', ':6', ':8', ':8']\n",
      "['_흘림표', '_흘림표']\n",
      "\n",
      "태:1 황:3 -:4 니:6 배남_흘림표:8_흘림표\n",
      "['태', '황', '-', '니', '배남', '<pad>']\n",
      "[':1', ':3', ':4', ':6', ':8', '<pad>']\n",
      "['_흘림표', '_흘림표']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lbs = [\n",
    "  '임_노니로:1 중:3 -:5 무_느니-르_노니로:8_느니-르_노니로:8',\n",
    "  '황:1 태:3 -:5 -:7 배임_노니로:9_노니로:9',\n",
    "  '황:1 태:3 -_흘림표:5 -:8 니:9니:9',\n",
    "  '태:1 황:3 -:4 니:6 배남_흘림표:8_흘림표:8',\n",
    "  '태:1 황:3 -:4 니:6 배남_흘림표:8_흘림표',\n",
    "]\n",
    "\n",
    "for lb in test_lbs:\n",
    "  print(lb)\n",
    "  \n",
    "  lb = tokenizer(lb)[1:-1]  \n",
    "  lb_split = split_enc_seq(tokenizer, lb)\n",
    "  print([ tokenizer.vocab[g[0]] for g in lb_split ])\n",
    "  print([ tokenizer.vocab[g[1]] for g in lb_split ])\n",
    "  print([ tokenizer.vocab[orn] for g in lb_split for orn in g[2] ])\n",
    "  \n",
    "  print()\n",
    "\n",
    "# print(test_lbs[2])\n",
    "# get_notes_and_positions(test_lbs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청남_노네:2 -:4 니나*:6 청임:8\n",
      "청남:2 -:4 니나*:6 청임:8\n"
     ]
    }
   ],
   "source": [
    "tok_list_decode = lambda tknz, tl: [ tknz.vocab[t] for t in tl ]\n",
    "\n",
    "def make_category_lists(gl, num_ctg=6): # gl: [(note, pos, [orn])]\n",
    "  n_l, p_l, np_l, o_l, op_l, onp_l = [ [] for _ in range(num_ctg) ]\n",
    "  \n",
    "  for g in gl:\n",
    "    n, p, oo = g\n",
    "    \n",
    "    n_l.append(n)\n",
    "    p_l.append(p)\n",
    "    np_l.append((n, p))\n",
    "    \n",
    "    o_l += oo\n",
    "    \n",
    "    for o in oo:\n",
    "      op_l.append((o, p))\n",
    "      onp_l.append((o, n, p))\n",
    "  \n",
    "  n_l = filter_specials(n_l)\n",
    "  p_l = filter_specials(p_l)\n",
    "  \n",
    "  return n_l, p_l, np_l, o_l, op_l, onp_l\n",
    "\n",
    "def prepare_pairs(gt_gl, prd_gl, num_ctg=6): # x_gl: list of tuples\n",
    "  \n",
    "  gt_ctg_lists = make_category_lists(gt_gl, num_ctg=num_ctg)\n",
    "  prd_ctg_lists = make_category_lists(prd_gl, num_ctg=num_ctg)\n",
    "  \n",
    "  return list(zip( gt_ctg_lists, prd_ctg_lists ))\n",
    "\n",
    "\n",
    "for test_case in test_ls[0:1]:\n",
    "  label, pred = test_case[2:4]\n",
    "  \n",
    "  label_dec = tokenizer.decode( label )\n",
    "  pred_dec = tokenizer.decode( pred )\n",
    "  \n",
    "  print(label_dec)\n",
    "  print(pred_dec)\n",
    "  \n",
    "  label_split = split_enc_seq(tokenizer, label)\n",
    "  pred_split = split_enc_seq(tokenizer, pred)\n",
    "  \n",
    "  prepare_pairs(label_split, pred_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 1.0, 0.99999999995) (1.0, 1.0, 0.99999999995) [76, 127] [76, 127]\n",
      "(1.0, 1.0, 0.99999999995) (1.0, 1.0, 0.99999999995) [6, 7] [6, 7]\n",
      "(1.0, 1.0, 0.99999999995) (1.0, 1.0, 0.99999999995) [(76, 6), (127, 7)] [(76, 6), (127, 7)]\n",
      "(1.0, 0.5, 0.6666666666222222) (1.0, 0.5, 0.6666666666222222) [52, 58] [58]\n",
      "(1.0, 0.5, 0.6666666666222222) (1.0, 0.5, 0.6666666666222222) [(52, 6), (58, 7)] [(58, 7)]\n",
      "(1.0, 0.5, 0.6666666666222222) (1.0, 0.5, 0.6666666666222222) [(52, 76, 6), (58, 127, 7)] [(58, 127, 7)]\n"
     ]
    }
   ],
   "source": [
    "def filter_pos_list(pl):\n",
    "  pl_f = []\n",
    "  \n",
    "  for p in pl:\n",
    "    try:\n",
    "      int(p) # if str p cannot be cast into integer goto except block\n",
    "      pl_f.append(p)\n",
    "    except:\n",
    "      pass\n",
    "  \n",
    "  return pl_f\n",
    "\n",
    "def calc_metric(gt, prd): # +) [꾸밈](GT 꾸밈 0개 이상일 때), [꾸밈, 위치], [본음, 꾸밈, 위치]\n",
    "  gt_tok_dict = defaultdict(int)\n",
    "  \n",
    "  for tok in gt:\n",
    "    gt_tok_dict[tok] += 1\n",
    "  \n",
    "  num_match = 0\n",
    "  \n",
    "  for tok in prd:\n",
    "    if gt_tok_dict[tok] > 0:\n",
    "      num_match += 1\n",
    "      gt_tok_dict[tok] -= 1\n",
    "  \n",
    "  eps = 1e-10\n",
    "  \n",
    "  precision = num_match / len(prd) if len(prd) > 0 else None\n",
    "  recall = num_match / len(gt) if len(gt) > 0 else None\n",
    "  \n",
    "  f1 = 2 * (precision * recall) / (precision + recall + eps) if precision != None and recall != None else None\n",
    "  \n",
    "  return precision, recall, f1\n",
    "\n",
    "def calc_all_metrics(_tokenizer, gt, prd):\n",
    "  # gt_dec = _tokenizer.decode( gt )\n",
    "  # prd_dec = _tokenizer.decode( prd )\n",
    "  \n",
    "  gt_split = split_enc_seq(_tokenizer, gt)\n",
    "  prd_split = split_enc_seq(_tokenizer, prd)\n",
    "  \n",
    "  # (note_p, pos_p, note_pos_p, orn_p, orn_pos_p, orn_note_pos_p)\n",
    "  # ((gt_note, prd_note), (gt_pos, prd_pos), (gt_note_pos, prd_note_pos), (gt_orn, prd_orn), (gt_orn_pos, prd_orn_pos), (gt_orn_note_pos, prd_orn_note_pos))\n",
    "  pairs = prepare_pairs(gt_split, prd_split)\n",
    "  \n",
    "  # (ner, per, nper, oer, oper, onper)\n",
    "  metric_values = [ calc_metric(*p) for p in pairs]\n",
    "  \n",
    "  return pairs, metric_values\n",
    "\n",
    "test_index = 1\n",
    "label, pred = test_ls[test_index][2:4]\n",
    "\n",
    "((label_note, pred_note), (label_pos, pred_pos), (label_note_pos, pred_note_pos), (label_orn, pred_orn), (label_orn_pos, pred_orn_pos), (label_orn_note_pos, pred_orn_note_pos)), (ner, per, nper, oer, oper, onper) = calc_all_metrics(tokenizer, label, pred)\n",
    "\n",
    "print(ner, calc_metric(label_note, pred_note), label_note, pred_note)\n",
    "print(per, calc_metric(label_pos, pred_pos), label_pos, pred_pos)\n",
    "print(nper, calc_metric(label_note_pos, pred_note_pos), label_note_pos, pred_note_pos)\n",
    "print(oer, calc_metric(label_orn, pred_orn), label_orn, pred_orn)\n",
    "print(oper, calc_metric(label_orn_pos, pred_orn_pos), label_orn_pos, pred_orn_pos)\n",
    "print(onper, calc_metric(label_orn_note_pos, pred_orn_note_pos), label_orn_note_pos, pred_orn_note_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test new metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#5 0.5556\n",
      "황:2 -_미는표:5 배중:8 16\n",
      "황_미는표:2 -:5 배중:8 16\n",
      "NER: (1.0, 1.0, 1.0)\n",
      "[140, 4, 106] 3\n",
      "[140, 4, 106] 3\n",
      "PER: (1.0, 1.0, 1.0)\n",
      "[12, 15, 18] 3\n",
      "[12, 15, 18] 3\n",
      "NPER: (1.0, 1.0, 1.0)\n",
      "[(140, 12), (4, 15), (106, 18)] 3\n",
      "[(140, 12), (4, 15), (106, 18)] 3\n",
      "OER: (1.0, 1.0, 1.0)\n",
      "[57] 1\n",
      "[57] 1\n",
      "OPER: (0.0, 0.0, 0.0)\n",
      "[(57, 15)] 1\n",
      "[(57, 12)] 1\n",
      "ONPER: (0.0, 0.0, 0.0)\n",
      "[(57, 4, 15)] 1\n",
      "[(57, 140, 12)] 1\n",
      "\n",
      "#6 0.6667\n",
      "황_자출:5 6\n",
      "황_전성:5 6\n",
      "NER: (1.0, 1.0, 1.0)\n",
      "[140] 1\n",
      "[140] 1\n",
      "PER: (1.0, 1.0, 1.0)\n",
      "[15] 1\n",
      "[15] 1\n",
      "NPER: (1.0, 1.0, 1.0)\n",
      "[(140, 15)] 1\n",
      "[(140, 15)] 1\n",
      "OER: (0.0, 0.0, 0.0)\n",
      "[67] 1\n",
      "[68] 1\n",
      "OPER: (0.0, 0.0, 0.0)\n",
      "[(67, 15)] 1\n",
      "[(68, 15)] 1\n",
      "ONPER: (0.0, 0.0, 0.0)\n",
      "[(67, 140, 15)] 1\n",
      "[(68, 140, 15)] 1\n",
      "\n",
      "#7 0.75\n",
      "태:2 -:5 -:7 흘림표_니:9 19\n",
      "태:2 -:5 -:7 니:9 15\n",
      "NER: (0.75, 0.75, 0.75)\n",
      "[127, 4, 4, 141] 4\n",
      "[127, 4, 4, 87] 4\n",
      "PER: (1.0, 1.0, 1.0)\n",
      "[12, 15, 17, 19] 4\n",
      "[12, 15, 17, 19] 4\n",
      "NPER: (0.75, 0.75, 0.75)\n",
      "[(127, 12), (4, 15), (4, 17), (141, 19)] 4\n",
      "[(127, 12), (4, 15), (4, 17), (87, 19)] 4\n",
      "OER: (None, 0.0, None)\n",
      "[47] 1\n",
      "[] 0\n",
      "OPER: (None, 0.0, None)\n",
      "[(47, 19)] 1\n",
      "[] 0\n",
      "ONPER: (None, 0.0, None)\n",
      "[(47, 141, 19)] 1\n",
      "[] 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, test_case in list(enumerate(test_ls))[5:8]:\n",
    "  label, pred = test_case[2:4]\n",
    "\n",
    "  label_dec = tokenizer.decode( label )\n",
    "  pred_dec = tokenizer.decode( pred )\n",
    "\n",
    "  ( (label_note, pred_note), (label_pos, pred_pos), (label_note_pos, pred_note_pos), (label_orn, pred_orn), (label_orn_pos, pred_orn_pos), (label_orn_note_pos, pred_orn_note_pos) ), (ner, per, nper, oer, oper, onper) = calc_all_metrics(tokenizer, label, pred)\n",
    "  \n",
    "  round_tuple_el = lambda t: tuple([ round(el, 4) if el != None else None for el in t ])\n",
    "  \n",
    "  ner, per, nper, oer, oper, onper = ( round_tuple_el(mv) for mv in (ner, per, nper, oer, oper, onper) )\n",
    "\n",
    "  print(f'#{idx} {round(test_case[4], 4)}')\n",
    "  print(label_dec, len(label_dec))\n",
    "  print(pred_dec, len(pred_dec))\n",
    "  print(f'NER: {ner}')\n",
    "  print(label_note, len(label_note))\n",
    "  print(pred_note, len(pred_note))\n",
    "  print(f'PER: {per}')\n",
    "  print(label_pos, len(label_pos))\n",
    "  print(pred_pos, len(pred_pos))\n",
    "  print(f'NPER: {nper}')\n",
    "  print(label_note_pos, len(label_note_pos))\n",
    "  print(pred_note_pos, len(pred_note_pos))\n",
    "  print(f'OER: {oer}')\n",
    "  print(label_orn, len(label_orn))\n",
    "  print(pred_orn, len(pred_orn))\n",
    "  print(f'OPER: {oper}')\n",
    "  print(label_orn_pos, len(label_orn_pos))\n",
    "  print(pred_orn_pos, len(pred_orn_pos))\n",
    "  print(f'ONPER: {onper}')\n",
    "  print(label_orn_note_pos, len(label_orn_note_pos))\n",
    "  print(pred_orn_note_pos, len(pred_orn_note_pos))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1532\n",
      "6\n",
      "1532\n",
      "3\n",
      "\n",
      "1532\n",
      "6\n",
      "3\n",
      "\n",
      "ner (0.982909051348999, 0.9915578764142733, 0.9860182390363174)\n",
      "per (0.9830178416013925, 0.9896866840731069, 0.9854825294601532)\n",
      "nper (0.9736401218450828, 0.9817558746736292, 0.9769360896802187)\n",
      "oer (0.9779827798277979, 0.9449799196787148, 0.9752067824153218)\n",
      "oper (0.9402624026240257, 0.9088353413654616, 0.9376964432936863)\n",
      "opner (0.9365723657236568, 0.9040160642570279, 0.9335607939969812)\n"
     ]
    }
   ],
   "source": [
    "num_metrics = 6\n",
    "metric_ls = [ [] for _ in range(num_metrics) ] # ner, per, nper, oer, oper, onper\n",
    "\n",
    "for case in pred_list:\n",
    "  label, pred = case[2:4]\n",
    "  \n",
    "  _, metric_values = calc_all_metrics(tokenizer, label, pred)\n",
    "  \n",
    "  for m_idx, mv in enumerate(metric_values):\n",
    "    metric_ls[m_idx].append(mv)\n",
    "\n",
    "print(len(pred_list))\n",
    "print(len(metric_ls)) # should be 6\n",
    "print(len(metric_ls[1])) # should be the same as len(pred_list)\n",
    "print(len(metric_ls[0][0])) # 3\n",
    "print()\n",
    "\n",
    "avg_metric_ls = []\n",
    "\n",
    "for mv_ls in metric_ls:\n",
    "  avg_mv = []\n",
    "  \n",
    "  for subm_i in range(3):\n",
    "    subm_ls = [ subm_t[subm_i] for subm_t in mv_ls if subm_t[subm_i] != None ]\n",
    "    avg_subm = sum(subm_ls) / len(subm_ls)\n",
    "    avg_mv.append( avg_subm )\n",
    "  \n",
    "  avg_metric_ls.append(tuple(avg_mv))\n",
    "\n",
    "print(len(pred_list))\n",
    "print(len(avg_metric_ls)) # should be 6\n",
    "print(len(avg_metric_ls[1])) # 3\n",
    "print()\n",
    "\n",
    "metric_names = ('ner', 'per', 'nper', 'oer', 'oper', 'opner')\n",
    "for m_name, avg_mv in zip(metric_names, avg_metric_ls):\n",
    "  print(m_name, avg_mv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "1532\n",
      "6\n",
      "1532\n",
      "3\n",
      "\n",
      "1532\n",
      "6\n",
      "3\n",
      "\n",
      "ner (0.982909051348999, 0.9915578764142733, 0.9860182390363174)\n",
      "per (0.9830178416013925, 0.9896866840731069, 0.9854825294601532)\n",
      "nper (0.9736401218450828, 0.9817558746736292, 0.9769360896802187)\n",
      "oer (0.9779827798277979, 0.9449799196787148, 0.9752067824153218)\n",
      "oper (0.9402624026240257, 0.9088353413654616, 0.9376964432936863)\n",
      "opner (0.9365723657236568, 0.9040160642570279, 0.9335607939969812)\n",
      "COMPLETE: calculating metrics\n"
     ]
    }
   ],
   "source": [
    "def test(project_root_dir, exp_dir, csv_path):\n",
    "  conf = OmegaConf.load(exp_dir / '.hydra' / 'config.yaml')\n",
    "  device = torch.device(conf.general.device)\n",
    "  \n",
    "  model_dir = exp_dir / 'model'\n",
    "  \n",
    "  print('\\n...tokenizer loading...')\n",
    "  \n",
    "  tokenizer_vocab_fn = model_dir / f'{conf.general.model_name}_tokenizer.txt'\n",
    "  tokenizer = Tokenizer(vocab_txt_fn=tokenizer_vocab_fn)\n",
    "  \n",
    "  print('COMPLETE: load tokenizer')\n",
    "  \n",
    "  \n",
    "  print('\\n...data_set loading...')\n",
    "  \n",
    "  test_set = LabelStudioDataset(project_root_dir / conf.data_path.test, project_root_dir / 'jeongganbo-png/splited-pngs', remove_borders=conf.test_setting.remove_borders, is_valid=True)\n",
    "  \n",
    "  test_set.tokenizer = tokenizer\n",
    "  \n",
    "  test_loader = DataLoader(test_set, batch_size=1000, shuffle=False, collate_fn=pad_collate, num_workers=conf.dataloader.num_workers_load)\n",
    "  \n",
    "  print('COMPLETE: data_set loading')\n",
    "  \n",
    "  \n",
    "  print('\\n...model initializing...')\n",
    "  model = TransformerOMR(conf.model.dim, len(tokenizer.vocab), enc_depth=conf.model.enc_depth, dec_depth=conf.model.dec_depth, num_heads=conf.model.num_heads, dropout=conf.model.dropout)\n",
    "  model.load_state_dict(torch.load(model_dir / f'{conf.general.model_name}_HL_{conf.test_setting.target_metric}_best.pt', map_location='cpu')['model'])\n",
    "\n",
    "  tester = Trainer(model, \n",
    "                  None, #optimizer\n",
    "                  None, #loss_fn\n",
    "                  None, #train_loader\n",
    "                  test_loader, \n",
    "                  tokenizer,\n",
    "                  device=device, \n",
    "                  scheduler=None,\n",
    "                  aux_loader=None,\n",
    "                  aux_freq=None,\n",
    "                  mix_aux=None,\n",
    "                  aux_valid_loader=None,\n",
    "                  wandb=None, \n",
    "                  model_name=conf.general.model_name,\n",
    "                  model_save_path=model_dir,\n",
    "                  checkpoint_logger=None)\n",
    "\n",
    "  print('COMPLETE: model initializing')\n",
    "  \n",
    "\n",
    "  print('\\n...testing...')\n",
    "  \n",
    "  _, metric_dict, pred_tensor_list, _ = tester.validate(with_confidence=True)\n",
    "  \n",
    "  print('COMPLETE: Testing')\n",
    "  \n",
    "  \n",
    "  print('\\n...processing test result...')\n",
    "  \n",
    "  pred_list = []\n",
    "\n",
    "  for b_idx in range(len(pred_tensor_list)):\n",
    "    pred_list += pred_tensor_list[b_idx].tolist()\n",
    "\n",
    "  pred_list = list( enumerate(pred_list) )\n",
    "  pred_list = make_gt_pred_tuple_list(tokenizer, pred_list)\n",
    "  \n",
    "  print('COMPLETE: processing test result')\n",
    "  \n",
    "  print('\\n...calculating metrics...')\n",
    "  \n",
    "  num_metrics = 6\n",
    "  metric_ls = [ [] for _ in range(num_metrics) ] # ner, per, nper, oer, oper, onper\n",
    "  \n",
    "  for case in pred_list:\n",
    "    label, pred = case[2:4]\n",
    "    \n",
    "    _, metric_values = calc_all_metrics(tokenizer, label, pred)\n",
    "    \n",
    "    for m_idx, mv in enumerate(metric_values):\n",
    "      metric_ls[m_idx].append(mv)\n",
    "  \n",
    "  print(len(pred_list))\n",
    "  print(len(metric_ls)) # should be 6\n",
    "  print(len(metric_ls[1])) # should be the same as num_total_case\n",
    "  print(len(metric_ls[0][0])) # 3\n",
    "  print()\n",
    "  \n",
    "  avg_metric_ls = []\n",
    "  \n",
    "  for mv_ls in metric_ls:\n",
    "    avg_mv = []\n",
    "    \n",
    "    for subm_i in range(3):\n",
    "      subm_ls = [ subm_t[subm_i] for subm_t in mv_ls if subm_t[subm_i] != None ]\n",
    "      avg_subm = sum(subm_ls) / len(subm_ls)\n",
    "      avg_mv.append( avg_subm )\n",
    "    \n",
    "    avg_metric_ls.append(tuple(avg_mv))\n",
    "\n",
    "  print(len(pred_list))\n",
    "  print(len(avg_metric_ls)) # should be 6\n",
    "  print(len(avg_metric_ls[1])) # 3\n",
    "  print()\n",
    "  \n",
    "  exact_all = metric_dict[conf.test_setting.target_metric]\n",
    "  \n",
    "  metric_names = ('ner', 'per', 'nper', 'oer', 'oper', 'opner')\n",
    "  for m_name, avg_mv in zip(metric_names, avg_metric_ls):\n",
    "    print(m_name, avg_mv)\n",
    "  \n",
    "  with open(csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # writer.writerow([conf.general.model_name, metric_dict[conf.test_setting.target_metric]] + metric_values)\n",
    "    writer.writerow( [conf.general.model_name, exact_all] + [mv_t[2] for mv_t in avg_metric_ls] )\n",
    "  \n",
    "  print('COMPLETE: calculating metrics')\n",
    "\n",
    "project_root_dir = Path('.')\n",
    "output_dir = project_root_dir / 'outputs'\n",
    "dir_date = '2024-05-04'\n",
    "dir_time = '20-31-38'\n",
    "\n",
    "exp_dir = output_dir / dir_date / dir_time  \n",
    "\n",
    "csv_path = project_root_dir / 'test' / 'new_metric_test.csv'\n",
    "\n",
    "pred_list = test(project_root_dir, exp_dir, csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from test_er import test as test_func\n",
    "\n",
    "get_ts = lambda: datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "\n",
    "project_root_dir = Path('.')\n",
    "output_dir = project_root_dir / 'outputs'\n",
    "save_path = project_root_dir / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model paths\n",
    "dir_dates = ['2024-05-03', '2024-05-04']\n",
    "dir_times = [\n",
    "  [\n",
    "    '21-55-11', \n",
    "    '22-41-31', \n",
    "    '23-27-48', \n",
    "  ],\n",
    "  [\n",
    "    '00-14-10', \n",
    "    '01-01-26', \n",
    "    '01-48-46', \n",
    "    '02-36-04', \n",
    "    '03-22-26', \n",
    "    '04-08-48', \n",
    "    '04-55-03', \n",
    "    '05-42-17', \n",
    "    '06-29-30', \n",
    "    '07-16-44', \n",
    "    '08-02-58', \n",
    "    '08-49-17', \n",
    "    '09-35-37', \n",
    "    '10-22-54', \n",
    "    '11-10-10', \n",
    "    '11-57-33', \n",
    "    '12-43-58', \n",
    "    '13-30-12', \n",
    "    '14-16-32', \n",
    "    '15-03-40', \n",
    "    '15-50-51', \n",
    "    '16-38-09', \n",
    "    '17-24-27', \n",
    "    '18-10-43', \n",
    "    '18-57-02', \n",
    "    '19-44-22',     \n",
    "    '20-31-38', \n",
    "  ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.982909051348999, 0.9915578764142733, 0.9860182390363174)\n",
      "per (0.9830178416013925, 0.9896866840731069, 0.9854825294601532)\n",
      "nper (0.9736401218450828, 0.9817558746736292, 0.9769360896802187)\n",
      "oer (0.9792127921279209, 0.9461847389558232, 0.9764474772043417)\n",
      "oper (0.9414924149241487, 0.9100401606425701, 0.9389371380827061)\n",
      "opner (0.9378023780237797, 0.9052208835341363, 0.934801488786001)\n",
      "EMR (None, None, 0.8674934725848564)\n",
      "NMR (None, None, 0.9321148825065274)\n",
      "PMR (None, None, 0.9497389033942559)\n",
      "OrMR (None, None, 0.9425587467362925)\n",
      "dist (None, None, 0.3133159268929504)\n",
      "dist/len (None, None, 0.03863266798347821)\n",
      "insert (None, None, 0.07963446475195822)\n",
      "del (None, None, 0.17558746736292427)\n",
      "sub (None, None, 0.058093994778067884)\n",
      "COMPLETE: calculating metrics\n"
     ]
    }
   ],
   "source": [
    "csv_path = save_path / 'new_metric_test.csv'\n",
    "\n",
    "dir_date, dir_time = (dir_dates[1], dir_times[1][-1])\n",
    "\n",
    "exp_dir = output_dir / dir_date / dir_time  \n",
    "test_func(project_root_dir, exp_dir, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.984508268059182, 0.9798629242819845, 0.9810364460977339)\n",
      "per (0.9848999129677987, 0.9814186248912096, 0.9823008148880727)\n",
      "nper (0.9711270670147963, 0.9679503916449087, 0.9689254065717541)\n",
      "oer (0.9163274536408861, 0.7853413654618474, 0.9350902183750829)\n",
      "oper (0.9020805065581181, 0.7732931726907631, 0.9206077872266898)\n",
      "opner (0.8932609678878332, 0.7660642570281125, 0.9118233617760173)\n",
      "EMR (None, None, 0.7721932114882507)\n",
      "NMR (None, None, 0.9379895561357703)\n",
      "PMR (None, None, 0.95822454308094)\n",
      "OrMR (None, None, 0.8198433420365535)\n",
      "dist (None, None, 0.40404699738903394)\n",
      "dist/len (None, None, 0.05560255612618915)\n",
      "insert (None, None, 0.23172323759791122)\n",
      "del (None, None, 0.07180156657963446)\n",
      "sub (None, None, 0.10052219321148825)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9770778938207137, 0.9777088772845952, 0.9763346820946595)\n",
      "per (0.9805700609225414, 0.9819516971279372, 0.980677391169587)\n",
      "nper (0.9619669277632725, 0.9630113141862489, 0.9620322489610553)\n",
      "oer (0.8199191473722893, 0.8926706827309239, 0.9388676031871191)\n",
      "oper (0.8019110621095182, 0.875401606425703, 0.9200510855209957)\n",
      "opner (0.7949283351708927, 0.866967871485944, 0.9119625372029795)\n",
      "EMR (None, None, 0.7826370757180157)\n",
      "NMR (None, None, 0.935378590078329)\n",
      "PMR (None, None, 0.95822454308094)\n",
      "OrMR (None, None, 0.8178851174934726)\n",
      "dist (None, None, 0.4112271540469974)\n",
      "dist/len (None, None, 0.07605899237369049)\n",
      "insert (None, None, 0.10966057441253264)\n",
      "del (None, None, 0.1768929503916449)\n",
      "sub (None, None, 0.12467362924281984)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.98371409921671, 0.978959965187119, 0.978993402834116)\n",
      "per (0.9722584856396871, 0.9730744125326368, 0.9716795895807607)\n",
      "nper (0.9589969538729333, 0.9611074847693646, 0.9595767917407502)\n",
      "oer (0.9535960378983633, 0.8779116465863458, 0.95009648298875)\n",
      "oper (0.931847545219638, 0.8596385542168677, 0.9297478991116338)\n",
      "opner (0.9182816537467697, 0.8475903614457834, 0.9166760036874294)\n",
      "EMR (None, None, 0.8322454308093995)\n",
      "NMR (None, None, 0.9268929503916449)\n",
      "PMR (None, None, 0.9412532637075718)\n",
      "OrMR (None, None, 0.8890339425587467)\n",
      "dist (None, None, 0.38642297650130547)\n",
      "dist/len (None, None, 0.05130862850779144)\n",
      "insert (None, None, 0.17558746736292427)\n",
      "del (None, None, 0.10966057441253264)\n",
      "sub (None, None, 0.10117493472584857)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9856940818102699, 0.9912206266318536, 0.9872427180675923)\n",
      "per (0.9849543080939946, 0.9909268929503915, 0.9868014817581774)\n",
      "nper (0.9769038294168841, 0.9816253263707572, 0.9784386667375722)\n",
      "oer (0.8457255747126438, 0.9366465863453817, 0.9601485148023513)\n",
      "oper (0.8279454022988506, 0.9157630522088355, 0.9394389438462326)\n",
      "opner (0.8230962643678161, 0.9097389558232933, 0.9338696369158282)\n",
      "EMR (None, None, 0.8257180156657964)\n",
      "NMR (None, None, 0.9595300261096605)\n",
      "PMR (None, None, 0.9634464751958225)\n",
      "OrMR (None, None, 0.8550913838120104)\n",
      "dist (None, None, 0.3198433420365535)\n",
      "dist/len (None, None, 0.06521365517151496)\n",
      "insert (None, None, 0.060704960835509136)\n",
      "del (None, None, 0.18407310704960836)\n",
      "sub (None, None, 0.07506527415143603)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.985496705209499, 0.9919495213228895, 0.9874940985409026)\n",
      "per (0.981266318537859, 0.9857049608355091, 0.9826212987270582)\n",
      "nper (0.974084607733433, 0.9773933855526545, 0.9751965143442219)\n",
      "oer (0.8810724962630792, 0.9399598393574297, 0.9670275066056402)\n",
      "oper (0.8565956651718981, 0.9118473895582329, 0.9392842353854545)\n",
      "opner (0.8528587443946187, 0.9070281124497991, 0.9351434486361914)\n",
      "EMR (None, None, 0.8459530026109661)\n",
      "NMR (None, None, 0.956266318537859)\n",
      "PMR (None, None, 0.9601827676240209)\n",
      "OrMR (None, None, 0.8831592689295039)\n",
      "dist (None, None, 0.3035248041775457)\n",
      "dist/len (None, None, 0.059347023339209645)\n",
      "insert (None, None, 0.06527415143603134)\n",
      "del (None, None, 0.16840731070496084)\n",
      "sub (None, None, 0.06984334203655353)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9812057068258114, 0.9875543951261967, 0.9831494187068659)\n",
      "per (0.9844872632266496, 0.9892950391644908, 0.9863312135511838)\n",
      "nper (0.9720346885490492, 0.9775348128807659, 0.9740828579107653)\n",
      "oer (0.9766158315177922, 0.9334337349397591, 0.970961485126556)\n",
      "oper (0.9611111111111111, 0.9173694779116466, 0.9553615959612813)\n",
      "opner (0.9574074074074074, 0.9137550200803213, 0.9518287613812385)\n",
      "EMR (None, None, 0.8857702349869452)\n",
      "NMR (None, None, 0.943864229765013)\n",
      "PMR (None, None, 0.95822454308094)\n",
      "OrMR (None, None, 0.9386422976501305)\n",
      "dist (None, None, 0.2643603133159269)\n",
      "dist/len (None, None, 0.03728981415732689)\n",
      "insert (None, None, 0.07049608355091384)\n",
      "del (None, None, 0.12140992167101827)\n",
      "sub (None, None, 0.07245430809399478)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9822236727589206, 0.9780678851174937, 0.9781509612607779)\n",
      "per (0.9746409921671019, 0.976066144473455, 0.974648338992077)\n",
      "nper (0.9600630983463886, 0.9617275892080069, 0.9604187623609504)\n",
      "oer (0.790617013508579, 0.8804216867469878, 0.9178860304821425)\n",
      "oper (0.7709017889740775, 0.8597389558232932, 0.8955786656471211)\n",
      "opner (0.7665206279664104, 0.8549196787148595, 0.8906829653493632)\n",
      "EMR (None, None, 0.7630548302872062)\n",
      "NMR (None, None, 0.9432114882506527)\n",
      "PMR (None, None, 0.9543080939947781)\n",
      "OrMR (None, None, 0.7950391644908616)\n",
      "dist (None, None, 0.4614882506527415)\n",
      "dist/len (None, None, 0.07829087855053517)\n",
      "insert (None, None, 0.14425587467362924)\n",
      "del (None, None, 0.19255874673629242)\n",
      "sub (None, None, 0.12467362924281984)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9830504786771107, 0.9842689295039163, 0.9821769070292496)\n",
      "per (0.9795147954743257, 0.9818755439512616, 0.979505847308829)\n",
      "nper (0.9689295039164493, 0.9705504786771104, 0.9688772280315898)\n",
      "oer (0.8385903426791272, 0.853413654618474, 0.9347442680293531)\n",
      "oper (0.8241822429906536, 0.8385542168674699, 0.9184303350495476)\n",
      "opner (0.8158099688473514, 0.8293172690763053, 0.9087301586830941)\n",
      "EMR (None, None, 0.7741514360313316)\n",
      "NMR (None, None, 0.943864229765013)\n",
      "PMR (None, None, 0.9575718015665796)\n",
      "OrMR (None, None, 0.8139686684073107)\n",
      "dist (None, None, 0.4314621409921671)\n",
      "dist/len (None, None, 0.07537391979681934)\n",
      "insert (None, None, 0.14425587467362924)\n",
      "del (None, None, 0.17297650130548303)\n",
      "sub (None, None, 0.11422976501305483)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.986216275021758, 0.9848455178416018, 0.9839684140426554)\n",
      "per (0.9719321148825064, 0.974553959965187, 0.972512612084844)\n",
      "nper (0.9628481288076588, 0.9654373368146214, 0.9635254069490032)\n",
      "oer (0.9595327492699203, 0.9069277108433738, 0.9550304785554626)\n",
      "oper (0.9322069253233204, 0.8814257028112453, 0.9279075381257984)\n",
      "opner (0.9280350438047557, 0.8774096385542173, 0.9238940189033298)\n",
      "EMR (None, None, 0.860313315926893)\n",
      "NMR (None, None, 0.9425587467362925)\n",
      "PMR (None, None, 0.9445169712793734)\n",
      "OrMR (None, None, 0.9053524804177546)\n",
      "dist (None, None, 0.3622715404699739)\n",
      "dist/len (None, None, 0.04408651160014243)\n",
      "insert (None, None, 0.13642297650130547)\n",
      "del (None, None, 0.10443864229765012)\n",
      "sub (None, None, 0.12140992167101827)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9860966057441252, 0.9886422976501303, 0.9864744516661738)\n",
      "per (0.9847258485639689, 0.9872171453437771, 0.9852038191944298)\n",
      "nper (0.975380765883377, 0.9775239338555266, 0.9759528801134125)\n",
      "oer (0.8535780496848349, 0.9057228915662651, 0.9416354556321493)\n",
      "oper (0.8374490174267704, 0.8884538152610442, 0.923657927543136)\n",
      "opner (0.8344827586206895, 0.8838353413654619, 0.9199126091912408)\n",
      "EMR (None, None, 0.8218015665796344)\n",
      "NMR (None, None, 0.9556135770234987)\n",
      "PMR (None, None, 0.9627937336814621)\n",
      "OrMR (None, None, 0.8550913838120104)\n",
      "dist (None, None, 0.34464751958224543)\n",
      "dist/len (None, None, 0.062054412485413844)\n",
      "insert (None, None, 0.09856396866840732)\n",
      "del (None, None, 0.1514360313315927)\n",
      "sub (None, None, 0.09464751958224543)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9874114136516225, 0.9921018276762402, 0.9887672270659712)\n",
      "per (0.9843124456048739, 0.9900892080069627, 0.986287294755817)\n",
      "nper (0.9770219445480544, 0.9818429068755441, 0.9788249282857918)\n",
      "oer (0.8651111111111108, 0.9377510040160643, 0.9617537312942028)\n",
      "oper (0.8367777777777775, 0.9066265060240962, 0.930161691494618)\n",
      "opner (0.8330740740740739, 0.9018072289156626, 0.9260157545130796)\n",
      "EMR (None, None, 0.8335509138381201)\n",
      "NMR (None, None, 0.9621409921671018)\n",
      "PMR (None, None, 0.9634464751958225)\n",
      "OrMR (None, None, 0.868798955613577)\n",
      "dist (None, None, 0.3093994778067885)\n",
      "dist/len (None, None, 0.058705972978454174)\n",
      "insert (None, None, 0.07049608355091384)\n",
      "del (None, None, 0.17297650130548303)\n",
      "sub (None, None, 0.06592689295039164)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9875543951261966, 0.9907528285465623, 0.9882283765923907)\n",
      "per (0.9835400348128809, 0.9879242819843342, 0.9849942919941991)\n",
      "nper (0.9758485639686687, 0.9794495213228894, 0.9771272967905698)\n",
      "oer (0.9722764227642277, 0.9508032128514055, 0.9755459414419896)\n",
      "oper (0.9283739837398374, 0.9070281124497991, 0.9311701688849241)\n",
      "opner (0.9247154471544715, 0.9022088353413654, 0.9270498557413587)\n",
      "EMR (None, None, 0.889686684073107)\n",
      "NMR (None, None, 0.9608355091383812)\n",
      "PMR (None, None, 0.9634464751958225)\n",
      "OrMR (None, None, 0.9406005221932114)\n",
      "dist (None, None, 0.2584856396866841)\n",
      "dist/len (None, None, 0.03399121681787561)\n",
      "insert (None, None, 0.08289817232375979)\n",
      "del (None, None, 0.10574412532637076)\n",
      "sub (None, None, 0.06984334203655353)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9594375543951276, 0.9837902523933862, 0.9651276377610783)\n",
      "per (0.9464425587467363, 0.9527850304612704, 0.9482389636282196)\n",
      "nper (0.9350522193211496, 0.9409377719756316, 0.9367578243731167)\n",
      "oer (0.7794062382562946, 0.8278112449799198, 0.8968066491223179)\n",
      "oper (0.7525366403607666, 0.7986947791164658, 0.8658792650468667)\n",
      "opner (0.7450206689214581, 0.7886546184738956, 0.8566929133413097)\n",
      "EMR (None, None, 0.7310704960835509)\n",
      "NMR (None, None, 0.9053524804177546)\n",
      "PMR (None, None, 0.922976501305483)\n",
      "OrMR (None, None, 0.7845953002610966)\n",
      "dist (None, None, 0.814621409921671)\n",
      "dist/len (None, None, 0.19736058957808675)\n",
      "insert (None, None, 0.09660574412532637)\n",
      "del (None, None, 0.5045691906005222)\n",
      "sub (None, None, 0.21344647519582247)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9848346388163618, 0.9861945169712796, 0.983941710980772)\n",
      "per (0.9738685813751083, 0.9783072236727588, 0.9749612404798792)\n",
      "nper (0.9654482158398605, 0.9695169712793735, 0.9664837011931278)\n",
      "oer (0.8556600078339208, 0.8599397590361448, 0.9423601936986449)\n",
      "oper (0.8305914610262434, 0.8386546184738958, 0.9173932187989906)\n",
      "opner (0.8264786525656088, 0.833835341365462, 0.9125495376015372)\n",
      "EMR (None, None, 0.7904699738903395)\n",
      "NMR (None, None, 0.9484334203655352)\n",
      "PMR (None, None, 0.9530026109660574)\n",
      "OrMR (None, None, 0.8198433420365535)\n",
      "dist (None, None, 0.42297650130548303)\n",
      "dist/len (None, None, 0.07544773461491316)\n",
      "insert (None, None, 0.14360313315926893)\n",
      "del (None, None, 0.16906005221932116)\n",
      "sub (None, None, 0.11031331592689295)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9877610966057443, 0.982778503046128, 0.9836751984100451)\n",
      "per (0.9697019147084424, 0.9700826805918187, 0.9691831123129478)\n",
      "nper (0.960302436901654, 0.9604547432550046, 0.9598078886102686)\n",
      "oer (0.9706368899917286, 0.9288152610441771, 0.9665832289869676)\n",
      "oper (0.9367245657568238, 0.8954819277108437, 0.9322903629059226)\n",
      "opner (0.9323821339950372, 0.8906626506024099, 0.9279098873116591)\n",
      "EMR (None, None, 0.8766318537859008)\n",
      "NMR (None, None, 0.9379895561357703)\n",
      "PMR (None, None, 0.9464751958224543)\n",
      "OrMR (None, None, 0.9321148825065274)\n",
      "dist (None, None, 0.3348563968668407)\n",
      "dist/len (None, None, 0.04180904181528132)\n",
      "insert (None, None, 0.14556135770234988)\n",
      "del (None, None, 0.07571801566579635)\n",
      "sub (None, None, 0.11357702349869452)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9889344771851301, 0.991514360313316, 0.9893899217487082)\n",
      "per (0.9857811140121847, 0.9896214099216709, 0.9870563617781287)\n",
      "nper (0.9781751212234242, 0.9813424717145343, 0.9793259756863051)\n",
      "oer (0.8672149122807016, 0.943273092369478, 0.9639334154872239)\n",
      "oper (0.8326754385964911, 0.9049196787148593, 0.9252157829362793)\n",
      "opner (0.8288377192982455, 0.9001004016064256, 0.9209001232570974)\n",
      "EMR (None, None, 0.8302872062663186)\n",
      "NMR (None, None, 0.9634464751958225)\n",
      "PMR (None, None, 0.9654046997389034)\n",
      "OrMR (None, None, 0.8733681462140992)\n",
      "dist (None, None, 0.3113577023498694)\n",
      "dist/len (None, None, 0.05763564607588191)\n",
      "insert (None, None, 0.08289817232375979)\n",
      "del (None, None, 0.16514360313315926)\n",
      "sub (None, None, 0.06331592689295039)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9883034937212484, 0.990774586597041, 0.9886689861713763)\n",
      "per (0.9868472584856399, 0.9900674499564839, 0.9877554451142255)\n",
      "nper (0.9785341290563225, 0.9814186248912097, 0.979392097553939)\n",
      "oer (0.909205426356589, 0.9327309236947792, 0.9547324374805668)\n",
      "oper (0.8913759689922478, 0.9142570281124498, 0.9359618104187756)\n",
      "opner (0.8867248062015501, 0.9082329317269077, 0.9308050447424899)\n",
      "EMR (None, None, 0.860313315926893)\n",
      "NMR (None, None, 0.9614882506527415)\n",
      "PMR (None, None, 0.9660574412532638)\n",
      "OrMR (None, None, 0.8909921671018277)\n",
      "dist (None, None, 0.26762402088772846)\n",
      "dist/len (None, None, 0.04063270040894451)\n",
      "insert (None, None, 0.08159268929503917)\n",
      "del (None, None, 0.11945169712793734)\n",
      "sub (None, None, 0.06657963446475196)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9879227278378715, 0.991155352480418, 0.9885848738810047)\n",
      "per (0.9838881636205397, 0.9880983463881636, 0.9852535518813962)\n",
      "nper (0.9764997513365661, 0.979949956483899, 0.9777011862025794)\n",
      "oer (0.9721539721539719, 0.9460843373493976, 0.9751138715861263)\n",
      "oper (0.9316134316134314, 0.906124497991968, 0.9343685299730009)\n",
      "opner (0.9279279279279277, 0.9013052208835343, 0.9302277432237531)\n",
      "EMR (None, None, 0.8981723237597912)\n",
      "NMR (None, None, 0.9608355091383812)\n",
      "PMR (None, None, 0.9640992167101827)\n",
      "OrMR (None, None, 0.9425587467362925)\n",
      "dist (None, None, 0.23694516971279372)\n",
      "dist/len (None, None, 0.03247239005427075)\n",
      "insert (None, None, 0.07767624020887728)\n",
      "del (None, None, 0.0926892950391645)\n",
      "sub (None, None, 0.06657963446475196)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9809725848563968, 0.9698542210617929, 0.9724508700842859)\n",
      "per (0.9694408181026983, 0.9679286335944296, 0.9676154023903029)\n",
      "nper (0.9521540469973894, 0.9512946040034812, 0.9509281550573776)\n",
      "oer (0.8323885109599389, 0.8796184738955823, 0.922603901563772)\n",
      "oper (0.8051776266061975, 0.8535140562248995, 0.8944444443981524)\n",
      "opner (0.7945956160241869, 0.8420682730923694, 0.8829940627193624)\n",
      "EMR (None, None, 0.7774151436031331)\n",
      "NMR (None, None, 0.9295039164490861)\n",
      "PMR (None, None, 0.9471279373368147)\n",
      "OrMR (None, None, 0.8231070496083551)\n",
      "dist (None, None, 0.46279373368146215)\n",
      "dist/len (None, None, 0.07190695334077676)\n",
      "insert (None, None, 0.19386422976501305)\n",
      "del (None, None, 0.14033942558746737)\n",
      "sub (None, None, 0.12859007832898173)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9805809399477804, 0.9734986945169727, 0.9747212896848392)\n",
      "per (0.9577241079199306, 0.9593342036553524, 0.9577527889379817)\n",
      "nper (0.9437336814621411, 0.9459856396866843, 0.9442346346239608)\n",
      "oer (0.844644238205723, 0.8520080321285143, 0.9358646616061356)\n",
      "oper (0.8129350348027841, 0.8253012048192773, 0.9046365914323867)\n",
      "opner (0.8065545243619489, 0.819277108433735, 0.8978383458186734)\n",
      "EMR (None, None, 0.7617493472584856)\n",
      "NMR (None, None, 0.9118798955613577)\n",
      "PMR (None, None, 0.9301566579634465)\n",
      "OrMR (None, None, 0.8198433420365535)\n",
      "dist (None, None, 0.5300261096605744)\n",
      "dist/len (None, None, 0.08522645495172744)\n",
      "insert (None, None, 0.21018276762402088)\n",
      "del (None, None, 0.17428198433420367)\n",
      "sub (None, None, 0.14556135770234988)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9849869451697126, 0.9850413402959098, 0.9834210248203982)\n",
      "per (0.9747824194952133, 0.976523063533507, 0.974963030103046)\n",
      "nper (0.9656005221932118, 0.967352045256745, 0.9658723093937491)\n",
      "oer (0.9661622530474989, 0.9107429718875506, 0.9632696390164817)\n",
      "oper (0.9403110550651533, 0.8868473895582332, 0.9377070063212947)\n",
      "opner (0.9346364018495166, 0.8820281124497994, 0.9323991506952155)\n",
      "EMR (None, None, 0.8655352480417755)\n",
      "NMR (None, None, 0.9379895561357703)\n",
      "PMR (None, None, 0.9490861618798956)\n",
      "OrMR (None, None, 0.9157963446475196)\n",
      "dist (None, None, 0.3426892950391645)\n",
      "dist/len (None, None, 0.04429411614895801)\n",
      "insert (None, None, 0.13315926892950392)\n",
      "del (None, None, 0.1077023498694517)\n",
      "sub (None, None, 0.10182767624020887)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9795039164490861, 0.9885879025239337, 0.9820402363313906)\n",
      "per (0.9813098346388163, 0.9893059181897301, 0.9833975242420717)\n",
      "nper (0.9717145343777197, 0.9784812880765882, 0.9735126346903833)\n",
      "oer (0.9023782125047947, 0.9233935742971887, 0.9624635871336681)\n",
      "oper (0.8645953202915225, 0.8838353413654617, 0.9214731585046635)\n",
      "opner (0.8605677023398539, 0.8802208835341364, 0.9175197669110029)\n",
      "EMR (None, None, 0.8433420365535248)\n",
      "NMR (None, None, 0.9503916449086162)\n",
      "PMR (None, None, 0.9608355091383812)\n",
      "OrMR (None, None, 0.8903394255874674)\n",
      "dist (None, None, 0.3328981723237598)\n",
      "dist/len (None, None, 0.06973151418753089)\n",
      "insert (None, None, 0.08028720626631854)\n",
      "del (None, None, 0.17362924281984335)\n",
      "sub (None, None, 0.07898172323759792)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9815382941688428, 0.9920583115752828, 0.9852398000398952)\n",
      "per (0.9780678851174937, 0.9853024369016534, 0.9804146933220503)\n",
      "nper (0.9705287206266322, 0.9773716275021759, 0.9729489033825458)\n",
      "oer (0.8817022872140979, 0.9317269076305219, 0.9652119700256696)\n",
      "oper (0.8588301462317208, 0.9042168674698793, 0.9388196175748045)\n",
      "opner (0.854330708661417, 0.8993975903614455, 0.9340399002018066)\n",
      "EMR (None, None, 0.8302872062663186)\n",
      "NMR (None, None, 0.9386422976501305)\n",
      "PMR (None, None, 0.9484334203655352)\n",
      "OrMR (None, None, 0.8772845953002611)\n",
      "dist (None, None, 0.370757180156658)\n",
      "dist/len (None, None, 0.06688502312590393)\n",
      "insert (None, None, 0.06462140992167102)\n",
      "del (None, None, 0.22519582245430808)\n",
      "sub (None, None, 0.08093994778067885)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.988174499564839, 0.9922432550043515, 0.989316279116323)\n",
      "per (0.9876414273281116, 0.9913620539599651, 0.9888007734441178)\n",
      "nper (0.9801784160139252, 0.9832136640557008, 0.9812064132063352)\n",
      "oer (0.9774774774774773, 0.9483935742971886, 0.977460711282087)\n",
      "oper (0.9559787059787058, 0.9275100401606424, 0.9558726219530552)\n",
      "opner (0.9504504504504503, 0.9214859437751003, 0.9500827129376431)\n",
      "EMR (None, None, 0.9099216710182768)\n",
      "NMR (None, None, 0.9647519582245431)\n",
      "PMR (None, None, 0.9699738903394256)\n",
      "OrMR (None, None, 0.943864229765013)\n",
      "dist (None, None, 0.19321148825065274)\n",
      "dist/len (None, None, 0.027709395759618487)\n",
      "insert (None, None, 0.0587467362924282)\n",
      "del (None, None, 0.0783289817232376)\n",
      "sub (None, None, 0.05613577023498695)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9705885552654483, 0.9780787641427334, 0.9729040686115276)\n",
      "per (0.978084203655352, 0.9835182767624022, 0.9797125958834497)\n",
      "nper (0.9585563533507403, 0.9632832898172335, 0.9601298794995085)\n",
      "oer (0.7954114524080392, 0.8369477911646586, 0.9100438596019481)\n",
      "oper (0.7836556693211982, 0.8246987951807229, 0.8965789473217731)\n",
      "opner (0.7677284793325748, 0.8056224899598394, 0.8776315789016641)\n",
      "EMR (None, None, 0.7349869451697127)\n",
      "NMR (None, None, 0.9092689295039165)\n",
      "PMR (None, None, 0.9471279373368147)\n",
      "OrMR (None, None, 0.7943864229765013)\n",
      "dist (None, None, 0.5156657963446475)\n",
      "dist/len (None, None, 0.08545523951423577)\n",
      "insert (None, None, 0.11031331592689295)\n",
      "del (None, None, 0.23563968668407312)\n",
      "sub (None, None, 0.16971279373368145)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9765665796344647, 0.988239773716275, 0.980313344134432)\n",
      "per (0.9749129677980855, 0.9822998259355965, 0.9773214238123022)\n",
      "nper (0.9646214099216713, 0.9733681462140997, 0.9677457623113432)\n",
      "oer (0.8384393063583816, 0.8661646586345381, 0.9459417011701062)\n",
      "oper (0.8218689788053951, 0.8503012048192773, 0.9280876994119318)\n",
      "opner (0.8168593448940269, 0.8458835341365463, 0.9229425806786135)\n",
      "EMR (None, None, 0.7669712793733682)\n",
      "NMR (None, None, 0.9210182767624021)\n",
      "PMR (None, None, 0.9412532637075718)\n",
      "OrMR (None, None, 0.8204960835509139)\n",
      "dist (None, None, 0.5176240208877284)\n",
      "dist/len (None, None, 0.0932039481470253)\n",
      "insert (None, None, 0.11553524804177545)\n",
      "del (None, None, 0.29569190600522194)\n",
      "sub (None, None, 0.10639686684073107)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9869560487380331, 0.9819081810269805, 0.983084057610044)\n",
      "per (0.9716818973020018, 0.9718885987815489, 0.971130975879605)\n",
      "nper (0.961303307223673, 0.9619451697127939, 0.9612375149747022)\n",
      "oer (0.9451476793248945, 0.8822289156626508, 0.9372498935233594)\n",
      "oper (0.910337552742616, 0.850100401606426, 0.9030225627458802)\n",
      "opner (0.9052742616033754, 0.8452811244979923, 0.8981268624481225)\n",
      "EMR (None, None, 0.8466057441253264)\n",
      "NMR (None, None, 0.9392950391644909)\n",
      "PMR (None, None, 0.9497389033942559)\n",
      "OrMR (None, None, 0.8981723237597912)\n",
      "dist (None, None, 0.3681462140992167)\n",
      "dist/len (None, None, 0.045947927234884)\n",
      "insert (None, None, 0.16057441253263707)\n",
      "del (None, None, 0.0783289817232376)\n",
      "sub (None, None, 0.12924281984334204)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9856505657093124, 0.9878698868581374, 0.9856257464114787)\n",
      "per (0.9797867711053091, 0.9828764142732812, 0.9807088508616234)\n",
      "nper (0.970659268929504, 0.9734660574412537, 0.9716069214596501)\n",
      "oer (0.8683765501691093, 0.8979919678714858, 0.9456963162139117)\n",
      "oper (0.8396279594137542, 0.8664658634538154, 0.9136208445173482)\n",
      "opner (0.8351183765501691, 0.8604417670682731, 0.9083797543708843)\n",
      "EMR (None, None, 0.8133159268929504)\n",
      "NMR (None, None, 0.9484334203655352)\n",
      "PMR (None, None, 0.9608355091383812)\n",
      "OrMR (None, None, 0.8537859007832899)\n",
      "dist (None, None, 0.37467362924281983)\n",
      "dist/len (None, None, 0.06254854901664142)\n",
      "insert (None, None, 0.11945169712793734)\n",
      "del (None, None, 0.15274151436031333)\n",
      "sub (None, None, 0.10248041775456919)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.9882801815243067, 0.9911771105308964, 0.988917102574846)\n",
      "per (0.9883703220191471, 0.9911227154046995, 0.9891306763523444)\n",
      "nper (0.9805343155538977, 0.9825935596170583, 0.9811865896878095)\n",
      "oer (0.8900038226299695, 0.9162650602409639, 0.961652555046031)\n",
      "oper (0.8745221712538227, 0.8993975903614457, 0.9444184230586767)\n",
      "opner (0.8706995412844036, 0.8945783132530121, 0.9402149762325017)\n",
      "EMR (None, None, 0.8433420365535248)\n",
      "NMR (None, None, 0.9614882506527415)\n",
      "PMR (None, None, 0.9686684073107049)\n",
      "OrMR (None, None, 0.8727154046997389)\n",
      "dist (None, None, 0.2800261096605744)\n",
      "dist/len (None, None, 0.05122246783617924)\n",
      "insert (None, None, 0.08942558746736293)\n",
      "del (None, None, 0.1325065274151436)\n",
      "sub (None, None, 0.058093994778067884)\n",
      "COMPLETE: calculating metrics\n",
      "\n",
      "...tokenizer loading...\n",
      "COMPLETE: load tokenizer\n",
      "\n",
      "...data_set loading...\n",
      "COMPLETE: data_set loading\n",
      "\n",
      "...model initializing...\n",
      "COMPLETE: model initializing\n",
      "\n",
      "...testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPLETE: Testing\n",
      "\n",
      "...processing test result...\n",
      "COMPLETE: processing test result\n",
      "\n",
      "...calculating metrics...\n",
      "ner (0.982909051348999, 0.9915578764142733, 0.9860182390363174)\n",
      "per (0.9830178416013925, 0.9896866840731069, 0.9854825294601532)\n",
      "nper (0.9736401218450828, 0.9817558746736292, 0.9769360896802187)\n",
      "oer (0.9792127921279209, 0.9461847389558232, 0.9764474772043417)\n",
      "oper (0.9414924149241487, 0.9100401606425701, 0.9389371380827061)\n",
      "opner (0.9378023780237797, 0.9052208835341363, 0.934801488786001)\n",
      "EMR (None, None, 0.8674934725848564)\n",
      "NMR (None, None, 0.9321148825065274)\n",
      "PMR (None, None, 0.9497389033942559)\n",
      "OrMR (None, None, 0.9425587467362925)\n",
      "dist (None, None, 0.3133159268929504)\n",
      "dist/len (None, None, 0.03863266798347821)\n",
      "insert (None, None, 0.07963446475195822)\n",
      "del (None, None, 0.17558746736292427)\n",
      "sub (None, None, 0.058093994778067884)\n",
      "COMPLETE: calculating metrics\n"
     ]
    }
   ],
   "source": [
    "csv_path = save_path / f'metric_test_{get_ts()}.csv'\n",
    "\n",
    "with open(csv_path, 'a', newline='', encoding='utf-8') as f:\n",
    "  writer = csv.writer(f)\n",
    "  writer.writerow( ('name', 'EM', 'ner', 'per', 'nper', 'oer', 'oper', 'opner', 'EMR', 'NMR', 'PMR', 'OrMR', 'dist', 'dist/len', 'ins', 'del', 'sub') )\n",
    "\n",
    "for date_idx, dir_date in enumerate(dir_dates):\n",
    "  for dir_time in dir_times[date_idx]:\n",
    "    \n",
    "    exp_dir = output_dir / dir_date / dir_time  \n",
    "    test_func(project_root_dir, exp_dir, csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jeongganbo-omr-QQMBMPoZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
